{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching NWP...\n",
      "Simulations start at:  2016-01-01 18:00:00\n",
      "Sample time of Sims is:  0:05:00\n",
      "Sample time of simulation: 0:05:00\n",
      "missing  503  samples from  2016-12-31 00:00:00 to  2017-01-01 18:00:00\n",
      "Setting flags for those datapoints to 1\n",
      "(649, 12) (503, 13)\n",
      "missing  1079  samples from  2017-03-23 00:00:00 to  2017-03-26 18:00:00\n",
      "Setting flags for those datapoints to 1\n",
      "(649, 12) (1079, 13)\n",
      "replacing the fauly values with the mean of the array\n",
      "fetched NWP ... now fetching PV Data\n",
      "Loading generation data from  2016-01-0117:00:00  to  2017-12-3117:05:00\n",
      "fetched PV ... now concatenating\n"
     ]
    }
   ],
   "source": [
    "# Daniel loads his data\n",
    "from load_DER_data import load_dataset\n",
    "raw_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\danielcmaz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\danielcmaz\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1500 with 17 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 percent converted\n",
      "9 percent converted\n",
      "19 percent converted\n",
      "29 percent converted\n",
      "39 percent converted\n",
      "49 percent converted\n",
      "59 percent converted\n",
      "69 percent converted\n",
      "79 percent converted\n",
      "89 percent converted\n"
     ]
    }
   ],
   "source": [
    "from datasets_utils import datasets_from_data\n",
    "inp, ev, pdf = datasets_from_data(raw_data,\n",
    "                                  sw_len_samples=int(7*24*(60/5)),\n",
    "                                  fc_len_samples=int(1*24*(60/5)),\n",
    "                                  fc_steps=48,\n",
    "                                  fc_tiles=33,\n",
    "                                  target_dims=[0,1,2,3,4],\n",
    "                                  plot=True,\n",
    "                                 steps_to_new_sample=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10705, 2016, 17) to  (2677, 2016, 17)\n",
      "(10705, 48, 33) to  (2677, 48, 33)\n",
      "(10705, 48) to  (2677, 48)\n"
     ]
    }
   ],
   "source": [
    "sw_train = sw_inputs[:int(0.8*sw_inputs.shape[0]),:,:]\n",
    "sw_test = sw_inputs[int(0.8*sw_inputs.shape[0]):,:,:]\n",
    "pdf_train = pdf_targets[:int(0.8*sw_inputs.shape[0]),:,:]\n",
    "pdf_test = pdf_targets[int(0.8*sw_inputs.shape[0]):,:,:]\n",
    "ev_train = ev_targets[:int(0.8*sw_inputs.shape[0]),:]\n",
    "ev_test = ev_targets[int(0.8*sw_inputs.shape[0]):,:]\n",
    "\n",
    "print(sw_train.shape, 'to ', sw_test.shape)\n",
    "print(pdf_train.shape, 'to ', pdf_test.shape)\n",
    "print(ev_train.shape, 'to ', ev_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class proper_FFW(tf.keras.layers.Layer): \n",
    "    def __init__(self, units, dropout_rate=0.,\n",
    "                 initializer=tf.initializers.he_uniform(),\n",
    "                 bias_init=tf.initializers.zeros(),\n",
    "                 w_reg=[1e-6, 1e-6], concat_input=False, residual_connect=False):\n",
    "        super(proper_FFW, self, ).__init__()\n",
    "        \n",
    "        self.concat_input = concat_input\n",
    "        self.residual_connect = residual_connect\n",
    "        \n",
    "        self.FFW = keras.layers.Dense(units=units,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     bias_initializer=bias_init,\n",
    "                                     kernel_regularizer=keras.regularizers.l1_l2(l1=w_reg[0], l2=w_reg[1]))\n",
    "        self.drop = keras.layers.Dropout(rate=dropout_rate)\n",
    "        self.norm = keras.layers.experimental.LayerNormalization()\n",
    "        self.activation = keras.layers.ELU()\n",
    "        \n",
    "    \n",
    "    def call(self,data_in):\n",
    "        data_out = self.FFW(data_in)\n",
    "        data_out = self.norm(data_out)\n",
    "        data_out = self.activation(data_out)\n",
    "        data_out = self.drop(data_out)\n",
    "        \n",
    "        return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class proper_GRU(tf.keras.layers.Layer): \n",
    "    def __init__(self, units, dropout_rate=0.2,\n",
    "                 initializer=tf.initializers.he_normal(),\n",
    "                 bias_init=tf.initializers.zeros(),\n",
    "                 w_reg=[1e-6, 1e-6], concat_input=False, residual_connect=False):\n",
    "        super(proper_GRU, self, ).__init__()\n",
    "        \n",
    "        self.concat_input = concat_input\n",
    "        self.residual_connect = residual_connect\n",
    "        \n",
    "        self.GRU = keras.layers.GRU(units=units,\n",
    "                                    activation='tanh',\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     bias_initializer=bias_init,\n",
    "                                     kernel_regularizer=keras.regularizers.l1_l2(l1=w_reg[0], l2=w_reg[1]),\n",
    "                                    return_sequences=True)\n",
    "        self.drop = keras.layers.Dropout(rate=dropout_rate,)\n",
    "        self.norm = keras.layers.experimental.LayerNormalization()\n",
    "        self.activation = keras.layers.ELU()\n",
    "        \n",
    "    \n",
    "    def call(self,data_in):\n",
    "        data_out = self.GRU(data_in)\n",
    "        data_out = self.norm(data_out)\n",
    "        data_out = self.activation(data_out)\n",
    "        data_out = self.drop(data_out)\n",
    "        \n",
    "        return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 34272)             0         \n",
      "_________________________________________________________________\n",
      "proper_ffw (proper_FFW)      (None, 4096)              140390400 \n",
      "_________________________________________________________________\n",
      "proper_ffw_1 (proper_FFW)    (None, 2048)              8394752   \n",
      "_________________________________________________________________\n",
      "proper_ffw_2 (proper_FFW)    (None, 1024)              2100224   \n",
      "_________________________________________________________________\n",
      "proper_ffw_3 (proper_FFW)    (None, 512)               525824    \n",
      "_________________________________________________________________\n",
      "proper_ffw_4 (proper_FFW)    (None, 128)               65920     \n",
      "_________________________________________________________________\n",
      "proper_ffw_5 (proper_FFW)    (None, 48)                6288      \n",
      "=================================================================\n",
      "Total params: 151,483,408\n",
      "Trainable params: 151,483,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the FF model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "keras.backend.clear_session()\n",
    "model =  keras.Sequential([\n",
    "    keras.layers.InputLayer(sw_inputs[0].shape),\n",
    "    keras.layers.Flatten(),\n",
    "    proper_FFW(units=4*1024),\n",
    "    proper_FFW(units=2*1024),\n",
    "    proper_FFW(units=1024),\n",
    "    proper_FFW(units=512),\n",
    "    proper_FFW(units=128),\n",
    "    proper_FFW(units=ev_targets[0].shape[0], dropout_rate=0.0)\n",
    "  ])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "model.compile(loss=tf.losses.MSE,\n",
    "            optimizer=optimizer,\n",
    "            metrics=[tf.losses.MAE, tf.losses.MSE])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.fit(x=sw_train, y=ev_train,\n",
    "          batch_size=256, epochs=15,\n",
    "          validation_split = 0.15)\n",
    "\n",
    "model.evaluate(sw_test, ev_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 48, 714)           0         \n",
      "_________________________________________________________________\n",
      "proper_gru (proper_GRU)      (None, 48, 1024)          5347328   \n",
      "_________________________________________________________________\n",
      "proper_gru_1 (proper_GRU)    (None, 48, 1024)          6299648   \n",
      "_________________________________________________________________\n",
      "proper_gru_2 (proper_GRU)    (None, 48, 1024)          6299648   \n",
      "_________________________________________________________________\n",
      "proper_gru_3 (proper_GRU)    (None, 48, 512)           2363392   \n",
      "_________________________________________________________________\n",
      "proper_gru_4 (proper_GRU)    (None, 48, 512)           1576960   \n",
      "_________________________________________________________________\n",
      "proper_ffw (proper_FFW)      (None, 48, 248)           127720    \n",
      "_________________________________________________________________\n",
      "proper_ffw_1 (proper_FFW)    (None, 48, 1)             251       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 48)                0         \n",
      "=================================================================\n",
      "Total params: 22,014,947\n",
      "Trainable params: 22,014,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9099 samples, validate on 1606 samples\n",
      "Epoch 1/15\n",
      "9099/9099 [==============================] - 18s 2ms/sample - loss: 1.6795 - mean_absolute_percentage_error: 329.7287 - mean_squared_error: 1.2269 - val_loss: 1.5600 - val_mean_absolute_percentage_error: 277.9165 - val_mean_squared_error: 1.1080\n",
      "Epoch 2/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 1.3762 - mean_absolute_percentage_error: 316.3706 - mean_squared_error: 0.9248 - val_loss: 1.3781 - val_mean_absolute_percentage_error: 260.4238 - val_mean_squared_error: 0.9276\n",
      "Epoch 3/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 1.2404 - mean_absolute_percentage_error: 305.8053 - mean_squared_error: 0.7906 - val_loss: 1.2646 - val_mean_absolute_percentage_error: 230.2181 - val_mean_squared_error: 0.8157\n",
      "Epoch 4/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 1.1379 - mean_absolute_percentage_error: 302.3209 - mean_squared_error: 0.6898 - val_loss: 1.1174 - val_mean_absolute_percentage_error: 222.7241 - val_mean_squared_error: 0.6701\n",
      "Epoch 5/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 1.0522 - mean_absolute_percentage_error: 286.7859 - mean_squared_error: 0.6059 - val_loss: 1.0671 - val_mean_absolute_percentage_error: 225.1944 - val_mean_squared_error: 0.6216\n",
      "Epoch 6/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.9923 - mean_absolute_percentage_error: 275.0908 - mean_squared_error: 0.5477 - val_loss: 1.0808 - val_mean_absolute_percentage_error: 224.9587 - val_mean_squared_error: 0.6372\n",
      "Epoch 7/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.9536 - mean_absolute_percentage_error: 279.6823 - mean_squared_error: 0.5108 - val_loss: 1.0618 - val_mean_absolute_percentage_error: 222.9967 - val_mean_squared_error: 0.6199\n",
      "Epoch 8/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.9244 - mean_absolute_percentage_error: 280.3949 - mean_squared_error: 0.4834 - val_loss: 1.0264 - val_mean_absolute_percentage_error: 210.6382 - val_mean_squared_error: 0.5864\n",
      "Epoch 9/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.9078 - mean_absolute_percentage_error: 284.3564 - mean_squared_error: 0.4686 - val_loss: 1.0774 - val_mean_absolute_percentage_error: 232.9694 - val_mean_squared_error: 0.6392\n",
      "Epoch 10/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.8913 - mean_absolute_percentage_error: 277.2199 - mean_squared_error: 0.4540 - val_loss: 1.0563 - val_mean_absolute_percentage_error: 224.2603 - val_mean_squared_error: 0.6199\n",
      "Epoch 11/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.8727 - mean_absolute_percentage_error: 278.1247 - mean_squared_error: 0.4372 - val_loss: 1.0982 - val_mean_absolute_percentage_error: 234.1786 - val_mean_squared_error: 0.6637\n",
      "Epoch 12/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.8526 - mean_absolute_percentage_error: 272.3913 - mean_squared_error: 0.4189 - val_loss: 1.0127 - val_mean_absolute_percentage_error: 215.3423 - val_mean_squared_error: 0.5799\n",
      "Epoch 13/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.8421 - mean_absolute_percentage_error: 267.2000 - mean_squared_error: 0.4102 - val_loss: 0.9884 - val_mean_absolute_percentage_error: 213.3693 - val_mean_squared_error: 0.5575\n",
      "Epoch 14/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.8314 - mean_absolute_percentage_error: 265.9258 - mean_squared_error: 0.4013 - val_loss: 1.0344 - val_mean_absolute_percentage_error: 222.1490 - val_mean_squared_error: 0.6053\n",
      "Epoch 15/15\n",
      "9099/9099 [==============================] - 16s 2ms/sample - loss: 0.8223 - mean_absolute_percentage_error: 267.7039 - mean_squared_error: 0.3940 - val_loss: 0.9854 - val_mean_absolute_percentage_error: 213.1260 - val_mean_squared_error: 0.5581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9542812668747274, 339.40506, 0.5269157]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the FF model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "keras.backend.clear_session()\n",
    "w_reg=[1e-6, 1e-6]\n",
    "initializer=tf.initializers.he_normal()\n",
    "bias_init=tf.initializers.zeros()\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.InputLayer(sw_inputs[0].shape))\n",
    "model.add(keras.layers.Reshape([ev_targets[0].shape[0],int(sw_inputs[0].shape[1]*(sw_inputs[0].shape[0]/ev_targets[0].shape[0]))]))\n",
    "model.add(proper_GRU(units=1024))\n",
    "model.add(proper_GRU(units=1024))\n",
    "model.add(proper_GRU(units=1024))\n",
    "model.add(proper_GRU(units=512))\n",
    "model.add(proper_GRU(units=512))\n",
    "model.add(proper_FFW(units=248, dropout_rate=0.3))\n",
    "model.add(proper_FFW(units=1, dropout_rate=0.0))\n",
    "model.add(keras.layers.Reshape([48]) )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "model.compile(loss=tf.losses.MSE,\n",
    "            optimizer=optimizer,\n",
    "            metrics=[tf.losses.MAPE, tf.losses.MSE])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x=sw_train, y=ev_train,\n",
    "          batch_size=256, epochs=15,\n",
    "          validation_split = 0.15)\n",
    "\n",
    "model.evaluate(sw_test, ev_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     10,
     14
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     1,
     7
    ]
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     1,
     14
    ]
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-04979cff5467>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0menc_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_hidden_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "import time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
